---
title: "Trabalho em Andamento"
author: "Danilo Pereira Mori"
date: "2024-02-26"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#21/08/2024

```{r}
p <- ggplot() +
  # Add text for footnotes
  annotate("text", x = 0.5, y = 1.05, label = "Footnote 1: AICc = Akaike Information Criterion corrected for small sample sizes.", size = 4, hjust = 0) +
  annotate("text", x = 0.5, y = 0.95, label = "Footnote 2: ΔAICc values < 2 indicate models with substantial support.", size = 4, hjust = 0) +
  # Set limits and theme to create space for the text
  xlim(0, 1) +
  ylim(0, 1) +
  theme_void() +   # Remove axes and grid
  theme(plot.margin = margin(1, 1, 2, 1, "cm"))  # Increase bottom margin for footnotes

# Save the plot as a PNG
ggsave("footnotes_plot.png", plot = p, width = 8, height = 4, dpi = 300)
```



# dia 20ago2024

```{r}
dfi <- df_tabelaSelecao %>% 
  select(-1) %>% 
  filter(Contraste == "Frag. total")
v_title <- dfi$Contraste[1] %>% 
  gsub("Frag. total","Frag. Total: contemporâneo / prístino",.) %>% 
  gsub("Frag. per se","Frag. per se: contemporâneo / aglomerado",.) %>% 
  gsub("Área per se","Área per se: aglomerado / prístino",.)
v_names <- names(dfi)[-1]
dfi <- dfi %>% select(-Contraste) %>%
  mutate(rank = 1:n())
table <- dfi %>%
  relocate(rank) %>% 
  gt() %>%
  tab_header(title = md(v_title)) %>%
  fmt_number(
    columns = v_names[-grep("modelo",v_names)],decimals = 2
  ) %>%
  tab_options(
    table.font.size = "small",
    table.align = "left"
  ) %>% 
  cols_label(
    rank = "Rank",
    modelo = "GAHM",
    df = "approx. parameters",
    dAICc = "ΔAICc",
    weight = "Weight (ΔAICc)",
    `Dev. explained` = "Deviance Explained",
    `Moran I statistic (res)` = "Moran's I",
    `p-value` = "p-value"
  )
plot <- dfi %>% 
  ggplot(aes(x = rank)) +
  geom_bar(aes(y = weight), stat = "identity", fill = "gray") +
  geom_point(aes(y = `p-value`, fill = `Moran I statistic (res)`), shape = 22, size = 5) +
  geom_segment(
    aes(x = rank - 0.4, xend = rank + 0.4, 
        y = `Dev. explained`, yend = `Dev. explained`,
        color="Deviance\nExplained"), 
    size = 1.5) +
  geom_hline(yintercept = 0.05,alpha=0.2,color="darkgreen") +
  scale_y_continuous("p-value (square), Weight (bar)",
                     limits = c(0, 1)) + # , sec.axis = dup_axis()
  scale_fill_gradientn(colours = c("cyan", "black", "red"),
                       values = c(-1,0,1),
                       name = "Moran's I\nStatistics") +
  scale_color_manual(values = c("Deviance\nExplained" = "blue"), name = "") +
  ## deixar para uma próxima:
  # new_scale_fill() +
  # scale_fill_manual(values = c("Weight (AICc)" = "gray"), name = "") +
  labs(x = "", fill = "Moran I statistic",) +
  theme_minimal() +
  theme(
    # axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "right"
  )
# saving the objects
gtsave(table, paste0(v_path,"tabelas/table_reciclagem.png"),
       vwidth = 800, vheight = 200)
ggsave(paste0(v_path,"figuras/plot_reciclagem.png"),plot = plot,bg = "white",
       width = 3, height = 3, units = "in", dpi = 300)
img <- image_read(paste0(v_path,"figuras/plot_reciclagem.png"))
trimmed_img <- image_trim(img)
image_write(trimmed_img, path = paste0(v_path,"figuras/plot_reciclagem.png"))
# combinação dos dois:
library(magick)
# Load the images
table_img <- image_read(paste0(v_path,"tabelas/table_reciclagem.png"))
plot_img <- image_read(paste0(v_path,"figuras/plot_reciclagem.png"))
plot_img <- image_resize(plot_img, geometry = paste0(image_info(table_img)$height, "x"))
# Combine the images vertically
combined_img <- image_append(c(table_img,plot_img), stack = FALSE)
# Save the combined image
v_name <- case_when(
  grepl("Total",v_title) ~ "fragtotal.png",
  grepl("Frag. per",v_title) ~ "fragperse.png",
  grepl("Área",v_title) ~ "areaperse.png",
)
image_write(combined_img, 
            path = paste0(v_path,folder_pattern,v_name), format = "png")
```


# antes do dia 20

```{r}
library(dagitty)
library(ggdag)
library(ggplot2)
library(dplyr)

# Define the DAG
cov_dag <- dagify(
  local ~ paisagem + biogeo + preservacao_local,
  paisagem ~ biogeo + U,
  preservacao_local ~ U,
  labels = c(
    "local" = "biodiv. local",
    "paisagem" = "efeito paisagem",
    "preservacao_local" = "preserv. local",
    "biogeo" = "Reg. biogeo.",
    "U" =  "unobs. cov."
  ),
  latent = "U",
  exposure = "paisagem",
  outcome = "local",
  coords = list(
    x = c(paisagem = 2.5, biogeo = 7.5, preservacao_local = 9, local = 5, U = 9),
    y = c(paisagem = 5, biogeo = 3.5, preservacao_local = 2.5, local = 1, U = 5.5)
  )
)

# Tidy the DAG and set the node class
tidy_cov_dag <- tidy_dagitty(cov_dag) %>%
  mutate(node_class = case_when(
    name == "paisagem" ~ "exposure",
    name == "local" ~ "outcome",
    name == "U" ~ "latent",
    TRUE ~ "adjusted"
  ))

# Split edge data based on parent node for custom coloring
edges_closed_path <- filter(tidy_cov_dag$data, name %in% c("biogeo", "preservacao_local"))
edges_open_path <- filter(tidy_cov_dag$data, !name %in% c("biogeo", "preservacao_local"))

# Create dummy data for the legend
legend_data <- data.frame(
  x = c(0, 0),
  y = c(1, 2),
  xend = c(1, 1),
  yend = c(1, 2),
  edge_type = c("closed path", "open path")
)

# Plot the DAG with custom arrow colors and manually created legends
ggplot() +
  geom_dag_edges_arc(data = edges_closed_path, 
                     aes(x = x, y = y, xend = xend, yend = yend), 
                     curvature = 0, color = "gray", show.legend = FALSE) +
  geom_dag_edges_arc(data = edges_open_path, 
                     aes(x = x, y = y, xend = xend, yend = yend), 
                     curvature = 0, color = "blue", show.legend = FALSE) +
  geom_dag_point(data = tidy_cov_dag, 
                 aes(x = x, y = y, fill = node_class, size = node_class), 
                 shape = 21) +
  geom_dag_label_repel(data = tidy_cov_dag, 
                       aes(x = x, y = y, label = label), 
                       colour = "black", show.legend = FALSE) +
  # Add dummy geom_segment for the legend
  geom_segment(data = legend_data,
               aes(x = x, y = y, xend = xend, yend = yend, color = edge_type),
               arrow = arrow(length = unit(0.2, "inches"), type = "closed"),
               size = 1.2) +
  scale_color_manual("Edge Type", values = c(
    "closed path" = "gray", 
    "open path" = "blue")) +
  scale_fill_manual("", values = c(
    "exposure" = "darkblue", 
    "outcome" = "darkgreen", 
    "latent" = "black", 
    "adjusted" = "darkred")) +
  scale_size_manual("", values = c(
    "exposure" = 10, 
    "outcome" = 10, 
    "latent" = 7, 
    "adjusted" = 7)) +
  theme_dag() +
  theme(legend.position = "right")

```



# 13ago2024
```{r}
library(ggplot2)
library(dplyr)

# Define the points
point1 <- c(x = 2, y = 5.8)  # Midpoint of one shorter side
point2 <- c(x = 0.2, y = 5.5)  # Midpoint of the other shorter side

# Calculate the distance between the points
distance <- sqrt((point2['x'] - point1['x'])^2 + (point2['y'] - point1['y'])^2)

# Define rectangle dimensions
short_side <- 3
long_side <- 7

# Calculate unit vector for the direction of the line
direction <- c(point2['x'] - point1['x'], point2['y'] - point1['y'])
direction <- direction / sqrt(sum(direction^2))  # Normalize

# Calculate the perpendicular direction
perpendicular <- c(-direction[2], direction[1])

# Calculate the offset to determine the other corners
half_short_side <- short_side / 2
half_long_side <- long_side / 2

# Coordinates of the corners
corner1 <- point1 + half_short_side * perpendicular - (distance / 2) * direction
corner2 <- point1 - half_short_side * perpendicular - (distance / 2) * direction
corner3 <- point2 - half_short_side * perpendicular + (distance / 2) * direction
corner4 <- point2 + half_short_side * perpendicular + (distance / 2) * direction

# Create a data frame for the rectangle
rect_df <- data.frame(
  x = c(corner1[1], corner2[1], corner3[1], corner4[1], corner1[1]),
  y = c(corner1[2], corner2[2], corner3[2], corner4[2], corner1[2])
)

# Example data
data <- data.frame(
  x = c(1, 2, 0.2, 3),
  y = c(3, 5.8, 5.5, 4)
)

# Plot the data with the rectangle
ggplot(data, aes(x = x, y = y)) +
  geom_point() +
  geom_polygon(data = rect_df, aes(x = x, y = y), fill = "blue", alpha = 0.2) +
  theme_minimal()

```



# 9ago2024

SCORE:
-> BUROCRACIAS MESTRADO
- agilização do 3o comitê: o que apresentar?
Entendo que podemos dividir o argumento em duas partes: da introdução até os resultados e da discussão até a conclusão. Então gostaria de que o terceiro comitê fosse sobre esse ponto de contato entre essas duas partes.
Me proponho a apresentar o suficiente da introdução para que os resultados sejam compreendidos e então gostaria de conversar sobre os caminhos que penso para a discussão. 



-> TRABALHO DE VERDADE NA DISSERTAÇÃO
- finalização dos resultados: 
a) criação de figuras e tabelas
1) Qual o status dos resultados?
2) 

b) texto explicando os resultados




# 1ago2024

Apresentação das tabelas:
Esquema proposto



# 31/07/2024

Por algum motivo não foi possível utilizar o output de predict.gam informando novos coeficientes, vou tentar utilizar


# 27 jul 2024

```{r}
coef_samples <- MASS::mvrnorm(n=nsim, mu=coef(gamm), Sigma=vcov(gamm))
predictions <- matrix(NA, nrow=nsim, ncol=nrow(data))
f_adply <- \(i){
  coef_i <- coef_samples[i,]
  predictions[i,] <- 
    predict(gamm, newdata=data, type="link", se.fit=FALSE, new.coef=coef_i,exclude=v_exclude)
  return(predictions)
}
predictions <- adply(1:nrow(predictions),1,f_adply,.parallel = TRUE)

df_pred <- apply(predictions,2,\(X) quantile(X,probs = quantiles)) %>% 
    as.data.frame()
  names(df_pred) <- paste0("Q_",quantiles)
  cbind(data,df_pred)
# exemplo do chatgpt
prediction_median <- apply(predictions, 2, median)
prediction_lower <- apply(predictions, 2, function(x) quantile(x, 0.025))
prediction_upper <- apply(predictions, 2, function(x) quantile(x, 0.975))

to_exclude = c("s(lat,long)",
               "s(data_year)")

nsim <- 100
for (i in 1:nsim) {
    coef_i <- coef_samples[i,]
    predictions[i,] <- predict(gamm, newdata=data, type="link", se.fit=FALSE, new.coef=coef_i)
}


dfmd <- gamm$model
dfmd$lat <- 0
dfmd$long <- 0

# Predict using the modified data frame
predictions <- predict.gam(gamm,
                           newdata = dfmd,
                           type = "link",
                           se.fit = TRUE)

# Check the predictions
print(predictions)
```
```{r}
df_newpred$pred <- 
      predict.gam(gamm, 
              newdata=df_newpred, 
              type="link", se.fit=FALSE, exclude=to_exclude)


matrix_lprediction <- predict(gamm,
                              type="lpmatrix",
                              exclude = to_exclude,
                              newdata= df_newpred,
                              newdata.guaranteed=TRUE)   
predict_link <- matrix_lprediction %*% t(coef_samples)
  df_pred <- t(apply(predict_link,1,\(X) quantile(X,probs = quantiles))) %>% 
    as.data.frame()
  names(df_pred) <- paste0("Q_",quantiles)
  cbind(data,df_pred)
```



# 25 jul 2024

Como plotar os efeitos parciais aleatórios? Melhor forma acredito que é fazer a predição a posteriori. Dúvida,
É possível fazer a predição a posterior dos efeitos aleatórios da paisagem? Acredito que se for pelo método gi certamente, agora se for pelo método gs eu já não sei dizer mesmo.

Quais são as CPF?
- fazer a tabela de seleção da 2a parte da análise de dados 



# 14 jul 2024

Moran test for spatial autocorrelation


```{r}
library(spdep)

# Example data
set.seed(123)
data <- data.frame(
  latitude = runif(100, -90, 90),
  longitude = runif(100, -180, 180),
  residuals = rnorm(100)
)

# Create coordinates matrix
coordinates <- as.matrix(data[, c("latitude", "longitude")])

# Define a range of k values (must be positive integers)
k_values <- 1:50
moran_results <- data.frame(k = k_values, Moran_I = NA, p_value = NA)

# Loop through k values and compute Moran's I
for (i in seq_along(k_values)) {
  k <- k_values[i]
  nb <- knn2nb(knearneigh(coordinates, k = k))
  listw <- nb2listw(nb)
  moran_test <- moran.test(data$residuals, listw)
  moran_results$Moran_I[i] <- moran_test$estimate[["Moran I statistic"]]
  moran_results$p_value[i] <- moran_test$p.value
}

# Plot the results
library(ggplot2)
ggplot(moran_results, aes(x = p_value, y = Moran_I,color=k)) +
  geom_line() +
  geom_point() +
  # labs(title = "Moran's I vs. k", x = "Number of Nearest Neighbors (k)", y = "Moran's I") +
  theme_minimal()

```

```{r}
library(spdep)
dfmd <- md$model
dfmd$residuals <- residuals(md)
dfmd_avgbySite <- dfmd %>% 
  group_by(SiteCode) %>% 
  summarise(mean_res = mean(residuals),
            lat = first(lat),
            long = first(long)) %>% 
  ungroup()
# Prepare spatial data
coordinates <- dfmd_avgbySite[, c("lat","long")]
coordinates <- as.matrix(coordinates)
nb <- knn2nb(knearneigh(coordinates, k=4))  
listw <- nb2listw(nb)
# Calculate Moran's I for residuals
moran_output <- moran.test(dfmd_avgbySite$mean_res, listw)
data.frame(
  Statistic = c(
    "Moran I statistic", 
    "Expectation", 
    "Variance", 
    "Standard Deviate", 
    "p-value"
    ),
  Value = c(
    moran_output$estimate[["Moran I statistic"]], 
    moran_output$estimate[["Expectation"]], 
    moran_output$estimate[["Variance"]], 
    moran_output$statistic, 
    moran_output$p.value
    )
) %>% pivot_wider(names_from=Statistic,values_from = Value)
```




# 21jun2024

```{r}
df_aud <- df_contrastes %>%
  select(SiteCode:p, contains("_logratio")) %>% 
  mutate(across(-SiteCode,f_z,.names = "{.col}_z"))
l_p <- list()
l_p$padrao <- 
  df_aud %>% 
  ggplot(aes(x=area_logratio_z,y=frag.total_logratio_z)) +
  geom_point() +
  labs(title="Escala Padrão") +
  geom_abline(slope=1,intercept = 0,color="darkred") +
  theme(aspect.ratio = 1)
l_p$z <- 
  df_aud %>% 
  ggplot(aes(x=area_logratio_z,y=frag.total_logratio_z)) +
  geom_point() +
  labs(title="z transformada") +
  geom_abline(slope=1,intercept = 0,color="darkred") +
  theme(aspect.ratio = 1)
grid.arrange(grobs=l_p,nrow=1)
df_aud %>% 
  select(area_logratio,frag.total_logratio) %>% 
  pivot_longer(cols = everything()) %>% 
  ggplot(aes(x=name,y=value)) +
  geom_boxplot() +
  geom_jitter()

```



# 20jun2024

```{r}
df_logOR %>% 
  select(-matches("^k|^p(!pristine)")) %>% 
  pivot_longer(cols = c("itself","pristine","numerator","denominator"),
               values_to="logOR",
               names_to="taxaU") %>% 
  mutate(taxaU = factor(taxaU,
                        levels=c("itself","pristine","numerator","denominator"))) %>% 
  ggplot(aes(x=Uefeito,y=logOR,color=taxaU,group=SiteCode)) +
  geom_hline(yintercept = 0,color="gray") +
  geom_vline(xintercept = 0,color="gray") +
  geom_line(alpha=0.2) +
  geom_point(alpha=0.2) +
  scale_color_manual(values=c("darkred","darkgreen","darkblue","darkorange")) +
  facet_grid(taxaU~contraste)
```



# 17/jun/2024

Os modelos para descrever os GAMMS pareciam precisar de maior grau de liberdade para descrever os padrões por sítios de amostragem. Interpretei que os dados sugerem que no caso 




# 15/06/2024

```{r}
v_num_itself <- dfi[dfi$taxaU==unname(cpair["numerador"]),
    c("SiteCode","k",cpair["numerador"],"taxaU")] %>% pull(contemp)
v_deno_itself <- dfi[dfi$taxaU==unname(cpair["denominador"]),
    c("SiteCode","k",cpair["denominador"],"taxaU")] %>% pull(ideal)
v_itself <- v_num_itself - v_deno_itself


dfi %>% head
dfi %>% filter(taxaU==uname(cpair["ideal"])) %>% pull(contemp)


```




# 4 mar 2024

Revisão dos resultados para avaliação da adequação


# 27fev2024




# 26fev2024

Anteriormente, 
simulei as SADs preditas nas paisagens contemporâneas e aglomeradas. Agora irei continuar as análises das SADs preditas nas paisagens contemporânea e aglomerada com o U prístino. 

Ontem fiz o pedido de prorrogação por conta da influência da pandemia.