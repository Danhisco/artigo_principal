---
title: "Contraste entre contrafactuais "
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup,include=FALSE}
knitr::opts_chunk$set(echo = FALSE,eval = TRUE,message = FALSE,warning = FALSE,cache = FALSE)
# pacotes
library(gratia)
library(doMC)
# library(metR)
# library(sjPlot)
library(gridExtra)
# library(ggpubr)
library(ggplot2)
library(readr)
library(purrr)
library(stringr)
library(tidyr)
# library(MuMIn)
# library(AICcmodavg)
# library(insight)
library(bbmle)
library(DHARMa)
library(mgcv)
# library(lme4)
# library(gamm4)
library(plyr)
library(dplyr)
source("source/2samples_testes.R")
source("source/general_tools.R")
source("source/GAMMtools.R")
source("source/fig_tools.R")
f_extract_smoothers_f_formula <- \(formula_obj){
  formula_obj %>% 
    as.character() %>% gsub(", bs = \"cr\"","",.) %>% 
    gsub(" \\+ s\\(k_z, by \\= SiteCode\\) \\+ s\\(SiteCode, bs \\= \"re\"\\)","",.) %>% 
    str_split_1(pattern = " \\+ ") %>% 
    grep(pattern='s\\(k_z\\)|s\\(p_z\\)|ti\\(p_z, k_z\\)',value = TRUE,x=.)
}
### dados ###
# modelos dos contrastes
# l_md <- readRDS("dados/Rdata/l_md.contrastes.Rdata")
# 
df_dados_disponiveis <- read_csv(file = "dados/df_dados_disponiveis.csv")
# df_p
df_p <- read_csv("dados/df_p.csv")
# df_resultMNEE
df_resultados <- read_csv("dados/csv/resultados_MN/df_resultados.csv")
# df_sim
df_sim <- read_csv("dados/df_simulacao.csv")
# df_ad: dados completos, com todos os logOR e as proporções observadas para os 3 e as preditoras
f_z <- function(x) (x-mean(x))/sd(x)
# df_contrastes: contrastes por sítio e grau de limitação de dispersão
df_contrastes <- read_csv(file="dados/csv/taxaU/df_contrastes.csv") 
# df_md
df_md_Uefeito <- df_contrastes %>% select(SiteCode:p, contains("_logratio")) %>% 
  pivot_longer(-c(SiteCode:p)) %>% 
  mutate(name = gsub("_logratio","",name),
         across(c(p,k),f_z,.names = "{.col}_z"),
         SiteCode = factor(SiteCode)) %>% 
  rename("tp_efeito"="name","v_efeito"=value)
```

# Encaminhamentos

Contrastes área:
comparar o nível hierarquico necessário para modelar: incluir um sem spline por sítio
Para além disso,
prosseguir com as simulações, incluindo aquelas com o U trocado, na prática resta fazer:
contemporâneo <-> aglomerado, pois U livre foi feito e U prístino foi feito.



## Introdução geral 

Iremos descrever a distribuição do log da razão de risco de colonização por uma nova espécie necessária para obter a riqueza observada no equilíbrio na respectiva paisagem contrafactual.
São três pares de contrastes entre paisagens contrafacturais, que representam os efeitos causais previstos em cada ontologia. O efeito de perda e fragmentação de habitat é obtido pelo contraste entre as paisagens contrafactuais contemporânea e sem perda de cobertura florestal. O efeito de perda per se de habitat é obtido pelo contraste entre as paisagens contrafactuais de cobertura contemporânea aglomerada ao redor da parcela e sem perda de cobertura florestal. O efeito de fragmentação per se de habitat é obtida pelo contraaste entre as paisagens contrafactuais 

## Gráficos exploratórios

```{r graficos exploratorios, fig.height=10}
l_p <- list()
df_plot <- df_contrastes %>% select(SiteCode:p, contains("_logratio")) %>% 
  pivot_longer(-c(SiteCode:p)) %>% 
  mutate(name = gsub("_logratio","",name))
v_sites_RefNulo <- df_plot %>% filter(p>=0.975) %>% pull(SiteCode) %>% unique
v_range_RefNulo <- df_plot %>% filter(p>=0.975) %>% pull(value) %>% range
theme_set(theme_bw())
df_text <- df_plot %>% 
  mutate(maior_q = value > max(v_range_RefNulo),
         menor_q = value < min(v_range_RefNulo)) %>% 
  group_by(name) %>% 
  summarise(prop_maiorq = round(100 * sum(maior_q) / n(),2),
            prop_menorq = round(100 * sum(menor_q) / n(),2)) %>% 
  mutate(v_geomtext = case_when(grepl("area",name) ~ "Contraste Área:\n",
                                grepl("frag.perse",name) ~ "Contraste Frag. per se:\n",
                                grepl("frag.total",name) ~ "Contraste Frag. Total:\n"),
         v_geomtext = paste0(v_geomtext,prop_maiorq,"% a direita ",prop_menorq,"% a esquerda"))
l_p$fig1 <- df_plot %>% 
  mutate(
    label = case_when(
      grepl("area",name) ~ "Contraste Área per se: log(U aglomerado / U prístino)",
      grepl("total",name) ~ "Contraste Frag. Total: log(U contemporâneo / U prístino)",
      grepl("perse",name) ~ "Contraste Frag. per se: log(U contemporâneo / U aglomerado)")) %>%
  left_join(.,df_text) %>%
  ggplot(aes(x=value)) +
  geom_vline(xintercept = 0,color="darkblue",linetype=3) +
  geom_vline(xintercept = v_range_RefNulo,color="darkred") + 
  geom_histogram(bins = 120) +
  geom_boxplot(aes(y=-17.5),width=30,alpha=0.4) +
  labs(y="",x="",
       tag="a)",
       caption="contraste em paisagens com %CF>=0.95") +
  geom_text(aes(x=1.5,y=300,label=v_geomtext),alpha=0.01,size=4) + 
  theme(plot.margin = unit(c(0.1,0.25,0.25,0), "cm"),
        strip.text = element_text(size=10),
        plot.caption.position = "plot",
        plot.caption = element_text(hjust = 0.2, face= "italic",vjust=10)) +
  facet_wrap(~label,ncol=1)
##
l_p$fig2 <- df_md %>% 
  mutate(label=case_when(
    grepl("area",name) ~ "Área per se",
    grepl("total",name) ~ "Frag. Total",
    grepl("perse",name) ~ "Frag. per se")
  ) %>% 
  ggplot(aes(x=k,y=value,group=SiteCode,color=p)) +
  geom_boxplot(inherit.aes = FALSE,
               aes(x=k,y=value,group=k)) +
  geom_line(alpha=0.5) +
  geom_point(alpha=0.5) +
  scale_colour_gradient2("% CF",midpoint=0.5,
                         low="darkred",
                         mid = "blue",
                         high = "darkgreen") +
  labs(x="k (prop. de propágulos até a vizinhança imediata)",
       y="log(U/U)",
       tag="b)") +
  facet_wrap(~label,ncol=3)
# grid.arrange(grobs=l_p,ncol=1)
p <- arrangeGrob(grobs=l_p,
                 # ncol=1,
                 layout_matrix = rbind(c(1),
                                       c(1),
                                       c(1),
                                       c(2),
                                       c(2)))
ggsave(filename = "./figuras/GE_taxaU_contrastes.png",plot = p,
       width = 10,height=7)
```

__Figura 1__ Gráficos Exploratórios 


## Ajuste dos modelos

```{r}
library(knitr)
df <- data.frame(
  modelo = c("gi", "gs", 
             "gi - p * k","gs - p * k",
             "p * k + 1|Site", "p + k + 1|Site",
             "mgcv::s"),
  `p + k` = c("0", "0", "1", "1","0","1","s(p,bs='cr') + s(k,bs='cr')"),
  `p * k` = c("1", "1", "0", "0","1","0","`p + k` + ti(p,k)"),
  `gi` = c("1", "0", "1", "0","0","0","s(k, by=Site, bs='cr')\n+ s(Site,bs='re')"),
  `gs` = c("0", "1", "0", "1","0","0","s(k, Site, bs='fs',xt=list(bs='cr'))\n+ s(Site,bs='re')"),
  `1|Site` = c("0", "0", "0", "0","1","1","s(Site,bs='re')")
  )
names(df) <- c("modelo","p + k","p * k","gi","gs","1|Site")
knitr::kable(df, 
             caption = "GAMMs usados para descrever o log(U/U) dos contrastes entre contrafactuais previstos em cada prática ontológica")
```





```{r GAM usado, echo=TRUE,eval=FALSE}
f_gam <- function(df){
  l_md <- list()
  # 1) maior flexibidade por sítio e com interação de p e k
  l_md$gi <- gam(value ~
                   s(k_z,bs = "cr") + s(p_z,bs = "cr") +
                   ti(p_z,k_z) +
                   s(k_z, by=SiteCode, bs="cr") +
                   s(SiteCode,bs="re"),
                 data=df,method = "REML")
  # 2) menor flexibilidade por sítio e com interação de p e k
  l_md$ti <- gam(value ~
                   s(k_z,bs = "cr") + s(p_z,bs = "cr") +
                   ti(p_z,k_z) +
                   s(k_z, SiteCode, bs="fs",xt=list(bs="cr")) +
                   s(SiteCode,bs="re"),
                 data=df,method = "REML")
  # 3) maior flexibilidade por sítio e sem interação de p e k
  l_md$gi_s_ti <- gam(value ~
                        s(k_z,bs = "cr") + s(p_z,bs = "cr") +
                        s(k_z, by=SiteCode, bs="cr") +
                        s(SiteCode,bs="re"),
                      data=df,method = "REML")
  # 4) menor flexibilidade por sítio e sem interação de p e k
  l_md$s_ti <- gam(value ~
                     s(k_z,bs = "cr") + s(p_z,bs = "cr") +
                     s(k_z, SiteCode, bs="fs",xt=list(bs="cr")) +
                     s(SiteCode,bs="re"),
                   data=df,method = "REML")
  # 5) apenas intercepto por sítio e com interação de p e k
  l_md$ti_sfs <- gam(value ~
                       s(k_z,bs = "cr") + s(p_z,bs = "cr") +
                       ti(p_z,k_z) +
                       s(SiteCode,bs="re"),
                     data=df,method = "REML")
  # 6) apenas intercepto por sítio e sem interação de p e k
  l_md$s_ti_sfs <- gam(value ~
                         s(k_z,bs = "cr") + s(p_z,bs = "cr") +
                         s(SiteCode,bs="re"),
                       data=df,method = "REML")
  return(l_md)
}
# registerDoMC(3)
l_md <- df_md %>% 
  split(.,df_md$name) %>% 
  lapply(.,f_gam)
saveRDS(l_md,file="dados/Rdata/l_md_contrastes.rds")
```

## Seleção de Modelos

```{r tabela de seleção}
l_md <- readRDS("dados/Rdata/l_md_contrastes.rds")
if(!file.exists("5_resultados/df_tabsel_contrasteU.csv")){
  df_tabsel_contrasteU <- ldply(l_md,f_TabSelGAMM,.id="pair")
  df_tabsel_contrasteU <- df_tabsel_contrasteU %>% 
    mutate(modelo = case_when(
      grepl("^ti$",modelo)~"gs",
      modelo=="gi_s_ti"~"gi - p * k",
      modelo=="s_ti"~"gs - p * k",
      modelo=="ti_sfs"~"p * k + 1|Site",
      modelo=="s_ti_sfs"~"p + k + 1|Site",
      TRUE~modelo))
  write_csv(df_tabsel_contrasteU,file="5_resultados/df_tabsel_contrasteU.csv")
}else{
  df_tabsel_contrasteU <-
    read_csv("./5_resultados/df_tabsel_contrasteU.csv")
}
# arrumar os nomes
f_names_lmd <- \(li){
  names(li) <- case_when(
      grepl("^ti$",names(li))~"gs",
      names(li)=="gi_s_ti"~"gi - p * k",
      names(li)=="s_ti"~"gs - p * k",
      names(li)=="ti_sfs"~"p * k + 1|Site",
      names(li)=="s_ti_sfs"~"p + k + 1|Site",
      TRUE~names(li))
  return(li)
}
l_md <- lapply(l_md,f_names_lmd)
#
l_md_maisplausivel <- lapply(names(l_md),\(li){
  df_tabsel <- df_tabsel_contrasteU %>% filter(pair==li)
  lmd <- l_md[[li]]
  lmd[[df_tabsel$modelo[1]]]
})
names(l_md_maisplausivel) <- names(l_md)
```
```{r}

```


<!-- ## Diagnóstico -->

<!-- ### Área per se -->

```{r tabela de seleção dos modelos para área,include=FALSE}
df_tabsel_contrasteU %>% filter(pair=="area")
```
```{r diagnóstico modelo mais plausível área,eval=FALSE,include=FALSE}
i <- 1
names(l_md_maisplausivel)[i]
md <- l_md_maisplausivel[[i]]
v_draw <- formula(md)[3] %>% f_extract_smoothers_f_formula
print("k.check:")
print(k.check(md))
print("summary:")
print(summary(md))
print(gratia::appraise(md))
print(gratia::draw(md,select = v_draw))
```

<!-- ### Fragmentation per se -->

```{r tabela de seleção dos modelos para frag.perse,include=FALSE}
df_tabsel_contrasteU %>% filter(pair=="frag.perse")
```
```{r frag per se,eval=FALSE,include=FALSE}
i <- 2
names(l_md_maisplausivel)[i]
md <- l_md_maisplausivel[[i]]
v_draw <- formula(md)[3] %>% f_extract_smoothers_f_formula
print("k.check:")
print(k.check(md))
print("summary:")
print(summary(md))
print(gratia::appraise(md))
print(gratia::draw(md))

```
```{r}

```

<!-- ### Fragmentation total -->


```{r tabela de seleção dos modelos para frag.total,include=FALSE}
l_tabsel[["frag.total"]]
```
```{r frag total,eval=FALSE,include=FALSE}
i <- 3
names(l_md_maisplausivel)[i]
md <- l_md_maisplausivel[[i]]
print("k.check:")
print(k.check(md))
print("summary:")
print(summary(md))
print(gratia::appraise(md))
print(gratia::draw(md))
```


<!-- ## Predito para o observado por sítio de amostragem -->

```{r predito para o observado,eval=FALSE,echo=FALSE,include=FALSE}
f_predic_bySite <- \(md){
  df_plot <- cbind(md$model,
                   predict(md,se.fit=TRUE)) |> 
  inner_join(x=df_p,"SiteCode") |> 
  arrange(p) |> 
  mutate(label = paste0(SiteCode,", p=",round(p,2)))
  df_plot$label <- factor(df_plot$label,levels=unique(df_plot$label))
  return(df_plot)
}
l_pred <- lapply(names(l_md_maisplausivel),\(li){
  f_predic_bySite(l_md_maisplausivel[[li]]) %>% 
    mutate(tp_efeito = li)
})
names(l_pred) <- names(l_md_maisplausivel)
#
theme_set(theme_bw())
f_plot_pred_bysite <- \(dfp,
                        path_plot = "./figuras/contrastes/"){
  v_title <- dfp$tp_efeito[1]
  v_title <- case_when(v_title=="area" ~ "Contraste Área",
                       v_title=="frag.perse" ~ "Contraste Frag. per se",
                       v_title=="frag.total" ~ "Contraste Frag. total")
  p <- dfp %>% 
    ggplot(aes(x=k_z,y=value)) +
    geom_hline(yintercept = 0,color="lightblue") +
    geom_line(aes(y=fit),color="darkred") +
    geom_line(aes(y=fit+se.fit)) +
    geom_line(aes(y=fit-se.fit)) +
    geom_point(alpha=0.5) +
    scale_x_reverse() +
    scale_y_continuous(labels = function(x) format(x, scientific = TRUE)) +
    labs(title=v_title,y="") +
    facet_wrap(~label,ncol=4) +
    theme(strip.text=element_text(margin=margin()),
          panel.spacing=unit(0, "lines"))
  ggsave_path <- paste0(path_plot,gsub("\\.","_",dfp$tp_efeito[1]),".png")
  ggsave(filename = ggsave_path, plot = p,width = 7,height = 30)
}
lapply(l_pred,f_plot_pred_bysite)
#
f_plot_alleffects_bysite <- \(l_pred,
                              path_plot = "./figuras/contrastes/"){
  df_pred <- rbind.fill(l_pred)
  p <- df_pred %>% 
    ggplot(aes(x=k_z,y=value,color=tp_efeito)) +
    geom_hline(yintercept = 0,color="lightblue") +
    geom_line(aes(y=fit)) +
    geom_point(alpha=0.5) +
    scale_color_manual("",values=c("darkgreen","darkblue","darkred")) +
    scale_x_reverse() +
    scale_y_continuous(labels = function(x) format(x, scientific = TRUE)) +
    labs(title="Todos os Efeitos",y="") +
    facet_wrap(~label,ncol=4) +
    theme(strip.text=element_text(margin=margin()),
          panel.spacing=unit(0, "lines"),
          legend.position = "top")
  ggsave_path <- paste0(path_plot,"todosEfeitos.png")
  ggsave(filename = ggsave_path, plot = p,width = 7,height = 30)
}
f_plot_alleffects_bysite(l_pred)
```



<!-- ### Estruura fixa do modelo -->

<!-- __5.6 Tensor product smooth interactions__ -->

<!-- The isotropy of the smooths considered above is often considered to be desirable -->
<!-- when modelling something as a smooth function of geographic coordinates,†† but it -->
<!-- has some disadvantages. Chief among them is the difficulty of knowing how to scale -->
<!-- predictors relative to one another, when both are arguments of the same smooth but -->
<!-- they are measured in fundamentally different units. For example, consider a smooth -->
<!-- function of a single spatial coordinate and time: the implied relative importance of -->
<!-- smoothness in time versus smoothness in space is very different between a situation -->
<!-- in which the units are metres and hours, compared to that in which the units are light- -->
<!-- years and nanoseconds. One pragmatic approach is to scale all predictors into the unit -->
<!-- square, as is often done in LOESS smoothing, but this is essentially arbitrary. A more -->
<!-- satisfactory approach uses tensor product smooths. Whereas the thin plate spline -->
<!-- generalizes from one dimension to multiple dimensions by exchanging a flexible -->
<!-- strip for a flexible sheet, the tensor product smooth can be interpreted as exchanging -->
<!-- a single flexible strip for a lattice work of strips (as is illustrated in figure 5.17), with -->
<!-- the component strips having different stiffness in different directions. -->

<!-- __5.6.1 Tensor product basis__ -->

<!-- The basic approach of this section is to start from smooths of single covariates, rep- -->
<!-- resented using any basis with an associated quadratic penalty measuring ‘wiggliness’ -->
<!-- of the smooth. From these ‘marginal smooths’ a ‘tensor product’ construction is used -->
<!-- to build up smooths of several variables. See de Boor (1978) for an important early -->
<!-- reference on tensor product spline bases, and Wood (2006a) for this construction. -->

<!-- The methods developed here can be used to construct smooth functions of any -->
<!-- number of covariates, but the simplest introduction is via the construction of a smooth -->
<!-- function of 3 covariates, x, z and v, the generalization then being trivial. The process -->
<!-- starts by assuming that we have low rank bases available for representing smooth -->
<!-- functions fx , fz and fv of each of the covariates. That is, we can write: -->

<!-- Continuing in the same way, we could now create a smooth function of x, z and v by allowing fxz to vary smoothly with v. -->
<!-- Again, the obvious way to do this is to let the parameters of fxz vary smoothly with -->
<!-- v, and following the same reasoning as before we get -->

<!-- FIGURA -->




<!-- ### Estrutura hierarquica do modelo:  -->

<!-- "to model intergroup variability using smooth curves" -->
<!-- I. Should each group have its own smoother, or will a common smoother suffice? -->
<!-- II. Do all of the group-specific smoothers have the same wiggliness, or should each group -->
<!-- have its own smoothing parameter? -->
<!-- III. Will the smoothers for each group have a similar shape to one another—a shared global -->
<!-- smoother? -->
<!-- These three choices result in five possible models (Fig. 4): -->
<!-- 1. A single common smoother for all observations; We will refer to this as model G, as it -->
<!-- only has a Global smoother. -->
<!-- 2. A global smoother plus group-level smoothers that have the same wiggliness. We will -->
<!-- refer to this as model GS (for Global smoother with individual effects that have a Shared -->
<!-- penalty). -->
<!-- 3. A global smoother plus group-level smoothers with differing wiggliness. We will refer to -->
<!-- this as model GI (for Global smoother with individual effects that have Individual -->
<!-- penalties). -->
<!-- 4. Group-specific smoothers without a global smoother, but with all smoothers having the -->
<!-- same wiggliness. We will refer to this as model S. -->
<!-- 5. Group-specific smoothers with different wiggliness. We will refer to this as model I. -->

<!-- __NOTA:__ O GAMM usado para ajustar os dados é do tipo 2: um smoother global mais um smoother no nível do grupo (sítio de amostragem) com mesmo grau de complexidade da forma do smoother. Assim, segue as notas desse tipo de modelo no resto da revisão -->

<!-- Model GS is a close analogue to a GLMM with varying slopes: all groups have similar -->
<!-- functional responses, but intergroup variation in responses is allowed. This approach -->
<!-- works by allowing each grouping level to have its own functional response, but penalizing -->
<!-- functions that are too far from the average. -->

<!-- This can be coded in mgcv by explicitly specifying one term for the global smoother (as in -->
<!-- model G above) then adding a second smooth term specifying the group-level smooth terms, -->
<!-- using a penalty term that tends to draw these group-level smoothers toward zero. mgcv -->
<!-- provides an explicit basis type to do this, the factor-smoother interaction or "fs" basis (see -->
<!-- ?mgcv::factor.smooth.interaction for details). This smoother creates a copy of each -->
<!-- set of basis functions for each level of the grouping variable, but only estimates one -->
<!-- smoothing parameter for all groups. To ensure that all parts of the smoother can be shrunk -->
<!-- toward zero effect, each component of the penalty null space is given its own penalty. -->

<!-- We modify the previous CO2 model to incorporate group-level smoothers as follows: -->
<!-- log eðuptakeiÞ ¼ f ðlog eðconciÞÞ þ fPlant uo i ðlogeðconciÞÞ þ ei -->
<!-- where fPlant_uoi(loge(conci)) is the smoother for concentration for the given plant. In R we -->
<!-- then have: -->

<!-- FIGURA  -->

<!-- Figure 10 shows the fitted smoothers for CO2_modGS. The plots of group-specific -->
<!-- smoothers indicate that plants differ not only in average log-uptake (which would -->
<!-- correspond to each plant having a straight line at different levels for the group-level -->
<!-- smoother), but differ slightly in the shape of their functional responses. Figure 11 shows -->
<!-- how the global and group-specific smoothers combine to predict uptake rates for -->
<!-- individual plants. We see that, unlike in the single global smoother case above, none of the -->
<!-- curves deviate from the data systematically. -->



